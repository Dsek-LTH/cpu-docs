# 🚨 2025-03-21

### 📋 Overview

| Incident | All services unavailable |
|----|----|
| Severity | Critical |
| Affected services | All services |
| Duration | March 21, 2025 1:50 until March 16, 2025 07:40 |
| Responders | @[Isak Kallini](mention://9d13a341-c3fd-4ff2-afea-190e23d334e4/user/6a2498b2-e61e-4c71-9755-6d7ff90efab3) @[David Agardh](mention://4363eee1-b4b6-414a-961b-cbefba05b639/user/b7aca933-2918-4648-8de7-a9692cd6e477)   |

### ⏱️ Incident timeline

* 01:51 — Report in #status-alerts that multiple services are down
* 07:31 — Thread created in #cpu-allmänt
* 07:35 — Manual startup of docker, mailmaster motion-generator and snejk
* 07:35 — Manual restart of pm2 on web-beta

### ⛑️ Postmortem report

* ⚠️ Leadup

  Around 01:46, something causes pando to reboot. The oVirt nodes begin logging errors because the nfs mounts are unreachable. A few minutes later, two of the oVirt nodes reboot, but not lavender. Pando comes back at 02:00 and nfs mounts are available again. Some VMs restart automatically, but not all.
* 🤷‍♀️ Fault

  !!Describe what didn't work as expected!!
* 🥏 Impact

  !!Describe how users where impacted during the incident. Include how many support cases were raised.!!
* 👁️ Detection

  System monitoring detected the outage immediately.
* 🔎 Root cause

  !!Run a 5-whys analysis to understand the true causes of the incident. Begin with a description of the impact and ask why it occurred. Continue asking "why" until you arrive at a root cause!!.
* 🗃️ Related records

  !!Check if any past incidents could've had the same root cause. Ask why this incident occured again.!!
* 🤔 Lessons learned

  !!Describe what you learned, what went well, and how you can improve.!!

### ✅ Follow-up tasks

List the issues created to prevent this class of incident in the future.

| Issue | Owner | Todo | Links |
|----|----|----|----|
|    |    |    |    |
|    |    |    |    |
|    |    |    |    |